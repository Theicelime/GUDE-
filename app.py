import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import os
import json
import re

# --- é…ç½®éƒ¨åˆ† ---
HISTORY_FILE = "search_history.json"
BASE_URL = "https://www.gooood.cn"

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
}

# --- è¾…åŠ©å‡½æ•° ---

def load_history():
    """åŠ è½½å†å²è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(record):
    """ä¿å­˜å†å²è®°å½•"""
    history = load_history()
    record['saved_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    history.insert(0, record)
    if len(history) > 20: # åªä¿ç•™æœ€è¿‘20æ¡
        history = history[:20]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def has_brackets(title):
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«æ‹¬å· (æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡)"""
    if not title: return False
    return (re.search(r'ï¼ˆ[^ï¼‰]*ï¼‰', title) is not None or 
            re.search(r'\([^)]*\)', title) is not None)

def contains_chinese(text):
    """æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    if not text: return False
    return bool(re.search(r'[\u4e00-\u9fff]+', text))

def check_comments_deeply(article_url, target_user="false"):
    """
    [æ ¸å¿ƒé€»è¾‘] è¿›å…¥æ–‡ç« è¯¦æƒ…é¡µï¼š
    1. è·å–çœŸå®è¯„è®ºæ•°
    2. æ£€æŸ¥æ˜¯å¦åªæœ‰ 'false' ç”¨æˆ·è¯„è®º
    """
    try:
        # éšæœºå»¶æ—¶ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«
        time.sleep(random.uniform(0.5, 1.2))
        
        resp = requests.get(article_url, headers=HEADERS, timeout=10)
        if resp.status_code != 200:
            return False, 0, "æ— æ³•è®¿é—®è¯¦æƒ…é¡µ"
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # æå–è¯„è®ºåŒºåŸŸ
        comment_elements = soup.select('.comment-body') 
        
        authors = []
        for c in comment_elements:
            # é€‚é…ä¸åŒ WordPress ä¸»é¢˜ç»“æ„
            author_tag = c.select_one('.fn') or c.select_one('.comment-author')
            if author_tag:
                authors.append(author_tag.get_text(strip=True))
        
        real_count = len(authors)
        
        if real_count == 0:
            return False, 0, "è¯¦æƒ…é¡µæ— è¯„è®º"

        # --- "false" ç”¨æˆ·æ’æŸ¥é€»è¾‘ ---
        unique_authors = set(authors)
        # å¦‚æœå»é‡åçš„ä½œè€…åªæœ‰ "false" (ä¸åŒºåˆ†å¤§å°å†™)ï¼Œåˆ™è§†ä¸ºæ— æ•ˆ
        if len(unique_authors) == 1 and target_user.lower() in [u.lower() for u in unique_authors]:
            return False, real_count, f"ä»…åŒ…å«ç”¨æˆ· {target_user}ï¼Œå·²å‰”é™¤"
            
        return True, real_count, "æœ‰æ•ˆæ¡ˆä¾‹"

    except Exception as e:
        return False, 0, f"è§£æé”™è¯¯: {str(e)}"

def scrape_logic_by_pages(start_page, end_page, min_comments, target_user_filter):
    """
    åŸºäºé¡µç èŒƒå›´çš„çˆ¬è™«é€»è¾‘
    """
    results = []
    
    # UI è¿›åº¦æ˜¾ç¤º
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    total_pages = end_page - start_page + 1
    
    # å¾ªç¯éå†æŒ‡å®šçš„é¡µç èŒƒå›´
    for i, page in enumerate(range(start_page, end_page + 1)):
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_percentage = (i) / total_pages
        progress_bar.progress(progress_percentage)
        
        # æ„å»º URL
        url = f"{BASE_URL}/page/{page}" if page > 1 else BASE_URL
        status_text.markdown(f"**ğŸ“¡ æ­£åœ¨æ‰«æç¬¬ {page} é¡µ...** ({i+1}/{total_pages})")
        
        try:
            resp = requests.get(url, headers=HEADERS, timeout=15)
            if resp.status_code != 200:
                st.warning(f"âš ï¸ ç¬¬ {page} é¡µæ— æ³•è®¿é—®ï¼Œå¯èƒ½å·²åˆ°è¾¾ç½‘ç«™æœ«å°¾ã€‚åœæ­¢ä»»åŠ¡ã€‚")
                break
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è·å–æ–‡ç« åˆ—è¡¨
            articles = soup.select('.post') 
            
            if not articles:
                st.info(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ«å°¾ã€‚")
                break

            for article in articles:
                # 1. æå–æ ‡é¢˜å’Œé“¾æ¥
                title_tag = article.select_one('h2 a') or article.select_one('h1 a')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag['href']

                # 2. æ ‡é¢˜æ¸…æ´—
                # A. å¿…é¡»åŒ…å«ä¸­æ–‡
                if not contains_chinese(title): continue
                # B. ä¸èƒ½åŒ…å«æ‹¬å·
                if has_brackets(title): continue

                # 3. è¯„è®ºæ•°åˆç­› (åˆ—è¡¨é¡µ)
                raw_comment_count = 0
                comment_tag = article.select_one('.comments-link')
                if comment_tag:
                    txt = comment_tag.get_text()
                    nums = re.findall(r'\d+', txt)
                    if nums:
                        raw_comment_count = int(nums[0])
                
                # 4. æ·±åº¦æ£€æŸ¥
                if raw_comment_count >= min_comments:
                    status_text.text(f"ğŸ” æ­£åœ¨æ·±åº¦æ£€æŸ¥: {title[:20]}...")
                    
                    is_valid, final_count, note = check_comments_deeply(link, target_user_filter)
                    
                    if is_valid and final_count >= min_comments:
                        results.append({
                            "é¡µç ": page,
                            "æ ‡é¢˜": title,
                            "é“¾æ¥": link,
                            "è¯„è®ºæ•°": final_count,
                            "çŠ¶æ€": note
                        })
            
            # é˜²å°å»¶æ—¶
            time.sleep(1)
            
        except Exception as e:
            st.error(f"ç¬¬ {page} é¡µæŠ“å–ä¸­æ–­: {e}")
            break
            
    progress_bar.progress(100)
    status_text.success(f"æŠ“å–å®Œæˆï¼èŒƒå›´: {start_page}-{end_page} é¡µ")
    return results

# --- Streamlit ç•Œé¢æ„å»º ---

st.set_page_config(page_title="Gooood æ¡ˆä¾‹ç­›é€‰ (é¡µç ç‰ˆ)", layout="wide", page_icon="ğŸ›ï¸")

st.title("ğŸ›ï¸ Gooood.cn æ¡ˆä¾‹ç­›é€‰å·¥å…· (é¡µç ç‰ˆ)")
st.markdown("""
é€šè¿‡æŒ‡å®š **é¡µç èŒƒå›´** ç›´æ¥æŠ“å–æ¡ˆä¾‹ã€‚
*   **æ ‡é¢˜æ¸…æ´—**ï¼šè‡ªåŠ¨å‰”é™¤æ— ä¸­æ–‡æˆ–å«æ‹¬å· `()` `ï¼ˆï¼‰` çš„æ ‡é¢˜ã€‚
*   **é»‘åå•**ï¼šè‡ªåŠ¨å‰”é™¤ä»…ç”±æŒ‡å®šç”¨æˆ·ï¼ˆå¦‚ `false`ï¼‰è¯„è®ºçš„æ¡ˆä¾‹ã€‚
""")

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("ğŸ› ï¸ ç­›é€‰è®¾ç½®")
    
    col_p1, col_p2 = st.columns(2)
    # é¡µç è¾“å…¥
    start_p = col_p1.number_input("èµ·å§‹é¡µç ", min_value=1, value=100, step=1)
    end_p = col_p2.number_input("ç»“æŸé¡µç ", min_value=1, value=110, step=1)
    
    st.caption(f"è®¡åˆ’æ‰«æ: **{end_p - start_p + 1}** ä¸ªé¡µé¢")
    
    st.markdown("---")
    
    # è¯„è®ºæ•°è®¾ç½®
    min_c = st.number_input("æœ€å°è¯„è®ºæ•° (N)", min_value=0, value=5)
    
    # ç”¨æˆ·è¿‡æ»¤
    target_user = st.text_input("å‰”é™¤å•ä¸€è¯„è®ºç”¨æˆ·", value="false")
    
    st.markdown("---")
    run_btn = st.button("ğŸš€ å¼€å§‹æŠ“å–", type="primary", use_container_width=True)

# --- ä¸»ç•Œé¢ ---

tab1, tab2 = st.tabs(["ğŸ“‹ ç»“æœåˆ—è¡¨", "ğŸ•’ å†å²è®°å½•"])

with tab1:
    if run_btn:
        if start_p > end_p:
            st.error("âŒ é”™è¯¯ï¼šèµ·å§‹é¡µç ä¸èƒ½å¤§äºç»“æŸé¡µç ï¼")
        else:
            with st.spinner(f'æ­£åœ¨æ‰«æç¬¬ {start_p} åˆ° {end_p} é¡µ...'):
                data = scrape_logic_by_pages(start_p, end_p, min_c, target_user)
            
            if data:
                df = pd.DataFrame(data)
                
                # ä¿å­˜å†å²
                save_history({
                    "criteria": f"Page: {start_p}-{end_p} | Min: {min_c}",
                    "count": len(data),
                    "data": data
                })
                
                st.success(f"âœ… å®Œæˆï¼å…±æ‰«æ {end_p - start_p + 1} é¡µï¼Œæ‰¾åˆ° {len(data)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ¡ˆä¾‹ã€‚")
                
                # å±•ç¤ºè¡¨æ ¼
                st.data_editor(
                    df,
                    column_config={
                        "é“¾æ¥": st.column_config.LinkColumn("ç‚¹å‡»æŸ¥çœ‹"),
                        "è¯„è®ºæ•°": st.column_config.NumberColumn("çƒ­åº¦", format="%d ğŸ’¬"),
                        "é¡µç ": st.column_config.NumberColumn("æ¥æºé¡µ", format="ç¬¬ %d é¡µ"),
                    },
                    hide_index=True,
                    use_container_width=True
                )
                
                # CSV ä¸‹è½½
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (CSV)", csv, "gooood_pages_result.csv", "text/csv")
                
            else:
                st.warning("âš ï¸ åœ¨æŒ‡å®šé¡µç èŒƒå›´å†…æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¡ˆä¾‹ã€‚")

with tab2:
    st.header("å†å²è®°å½•")
    history_data = load_history()
    
    if not history_data:
        st.caption("æš‚æ— å†å²è®°å½•")
    
    for i, record in enumerate(history_data):
        with st.expander(f"ğŸ“… {record['saved_at']} - {record['criteria']} (ç»“æœ: {record['count']})"):
            if record['data']:
                h_df = pd.DataFrame(record['data'])
                st.dataframe(
                    h_df,
                    column_config={"é“¾æ¥": st.column_config.LinkColumn("é“¾æ¥")},
                    hide_index=True,
                    use_container_width=True
                )
                h_csv = h_df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(f"ä¸‹è½½æ­¤è®°å½•", h_csv, key=f"hist_{i}")
