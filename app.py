import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import os
import json
import re

# --- é…ç½®éƒ¨åˆ† ---
HISTORY_FILE = "search_history.json"
BASE_URL = "https://www.gooood.cn"

# æ¨¡æ‹Ÿæ›´çœŸå®çš„æµè§ˆå™¨è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Connection": "keep-alive",
    "Referer": "https://www.gooood.cn/",
    "Upgrade-Insecure-Requests": "1"
}

# --- è¾…åŠ©å‡½æ•° ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(record):
    history = load_history()
    record['saved_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    history.insert(0, record)
    if len(history) > 20: 
        history = history[:20]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def has_brackets(title):
    if not title: return False
    return (re.search(r'ï¼ˆ[^ï¼‰]*ï¼‰', title) is not None or 
            re.search(r'\([^)]*\)', title) is not None)

def contains_chinese(text):
    if not text: return False
    return bool(re.search(r'[\u4e00-\u9fff]+', text))

def extract_comment_count_from_header(soup):
    """
    [æ–°å¢] ç›´æ¥ä»é¡µé¢çš„æ ‡é¢˜ï¼ˆå¦‚ '11 è¯„è®º'ï¼‰ä¸­æå–æ•°å­—
    è¿™æ˜¯æœ€å‡†ç¡®çš„æ¥æºï¼Œä¸ä¾èµ–å…·ä½“çš„è¯„è®º HTML ç»“æ„ã€‚
    """
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« "è¯„è®º" äºŒå­—çš„æ ‡é¢˜æ ‡ç­¾ (h1-h6, div, span)
    targets = soup.find_all(['h3', 'h2', 'h4', 'div', 'span'], string=re.compile(r'è¯„è®º|Comments'))
    
    for t in targets:
        text = t.get_text(strip=True)
        # å°è¯•åŒ¹é… "11 è¯„è®º", "11 Comments", "è¯„è®º: 11"
        match = re.search(r'(\d+)\s*(æ¡)?(è¯„è®º|Comments?)', text, re.IGNORECASE)
        if match:
            return int(match.group(1))
        
        # å°è¯•åŒ¹é… "è¯„è®º (11)"
        match2 = re.search(r'(è¯„è®º|Comments?)\s*[:\uff1a\(ï¼ˆ]\s*(\d+)', text, re.IGNORECASE)
        if match2:
            return int(match2.group(2))
            
    return 0

def fetch_detail_and_count(article_url, target_user="false"):
    """
    [è¶…çº§å¢å¼ºç‰ˆ] è¿›å…¥è¯¦æƒ…é¡µè·å–æ•°æ®
    """
    try:
        time.sleep(random.uniform(0.3, 0.8)) # éšæœºå»¶æ—¶
        
        resp = requests.get(article_url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            return None, 0, f"Error {resp.status_code}"
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # --- ç­–ç•¥ A: ç›´æ¥è¯»å–æ ‡é¢˜æ•°å­— (æƒå¨å‚è€ƒ) ---
        header_count = extract_comment_count_from_header(soup)
        
        # --- ç­–ç•¥ B: æŠ“å–ä½œè€…å (ç”¨äº false è¿‡æ»¤) ---
        # æ‰©å¤§æœç´¢èŒƒå›´ï¼Œä¸ä»…ä»…æ‰¾ comment-body
        authors = []
        
        # å¸¸è§çš„ä½œè€…å®¹å™¨ class
        author_selectors = [
            '.fn',                 # æ ‡å‡† WordPress
            '.comment-author',     # å¸¸è§
            '.comment-author .fn', 
            '.vcard .fn',
            'cite.fn',
            '.url'                 # æœ‰æ—¶å€™ä½œè€…ååœ¨ href class="url"
        ]
        
        for selector in author_selectors:
            tags = soup.select(selector)
            for tag in tags:
                name = tag.get_text(strip=True)
                if name:
                    authors.append(name)
            if authors: # å¦‚æœæ‰¾åˆ°äº†ä¸€ç§ï¼Œé€šå¸¸å°±å¤Ÿäº†ï¼Œä¸ç”¨æ··ç€æ‰¾
                break
        
        real_count_from_authors = len(authors)
        
        # --- å†³ç­–é€»è¾‘ ---
        
        # 1. ä¼˜å…ˆä½¿ç”¨ä½œè€…æ•°é‡ï¼Œå› ä¸ºå¯ä»¥è¿‡æ»¤ false
        final_count = max(header_count, real_count_from_authors)
        
        if final_count == 0:
            return True, 0, "æ— è¯„è®º"

        # 2. å¦‚æœæŠ“åˆ°äº†ä½œè€…åï¼Œæ‰§è¡Œ false è¿‡æ»¤
        if len(authors) > 0:
            unique_authors = set(authors)
            target_user_lower = target_user.lower()
            unique_lower = {u.lower() for u in unique_authors}
            
            if len(unique_lower) == 1 and target_user_lower in unique_lower:
                return False, final_count, f"ä»…å«ç”¨æˆ· {target_user}"
            
            return True, final_count, "æœ‰æ•ˆ"
        
        # 3. [å…œåº•] å¦‚æœæ²¡æŠ“åˆ°ä½œè€…åï¼Œä½†æ˜¯æ ‡é¢˜è¯´æœ‰è¯„è®º (header_count > 0)
        # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æ— æ³•åˆ¤æ–­æ˜¯ä¸æ˜¯ false ç”¨æˆ·ï¼Œä¸ºäº†ä¸æ¼æ‰ï¼Œæˆ‘ä»¬é»˜è®¤ä¿ç•™ï¼
        if header_count > 0 and len(authors) == 0:
            return True, header_count, f"æ˜¾ç¤º {header_count} æ¡è¯„è®º (æ— æ³•è¯»å–è¯¦æƒ…)"

        return True, final_count, "æœ‰æ•ˆ"

    except Exception as e:
        return None, 0, f"è§£æå¼‚å¸¸: {str(e)}"

def scrape_logic_strict(start_page, end_page, min_comments, target_user_filter):
    results = []
    status_text = st.empty()
    progress_bar = st.progress(0)
    log_area = st.empty()
    
    total_pages = end_page - start_page + 1
    stats = {"processed": 0, "hit": 0}
    
    for i, page in enumerate(range(start_page, end_page + 1)):
        progress_percentage = (i) / total_pages
        progress_bar.progress(progress_percentage)
        
        url = f"{BASE_URL}/page/{page}" if page > 1 else BASE_URL
        status_text.markdown(f"**ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...** (å·²å‘½ä¸­: {stats['hit']} ä¸ª)")
        
        try:
            resp = requests.get(url, headers=HEADERS, timeout=20)
            if resp.status_code != 200:
                st.warning(f"âš ï¸ ç¬¬ {page} é¡µè®¿é—®å¤±è´¥")
                continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            articles = soup.select('.post') 
            
            if not articles:
                # å°è¯•å¤‡ç”¨é€‰æ‹©å™¨
                articles = soup.select('article')
                
            if not articles:
                st.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ï¼Œåœæ­¢ã€‚")
                break

            for art in articles:
                t_tag = art.select_one('h2 a') or art.select_one('h1 a') or art.select_one('.entry-title a')
                if t_tag:
                    title = t_tag.get_text(strip=True)
                    link = t_tag['href']
                    
                    # æ ‡é¢˜æ¸…æ´—
                    if not contains_chinese(title): continue
                    if has_brackets(title): continue
                    
                    stats["processed"] += 1
                    log_area.text(f"æ£€æŸ¥ä¸­: {title[:20]}...")
                    
                    # å¼ºåˆ¶æ£€æŸ¥è¯¦æƒ…é¡µ
                    is_valid_user, real_count, note = fetch_detail_and_count(link, target_user_filter)
                    
                    if is_valid_user is True:
                        if real_count >= min_comments:
                            results.append({
                                "é¡µç ": page,
                                "æ ‡é¢˜": title,
                                "é“¾æ¥": link,
                                "è¯„è®ºæ•°": real_count,
                                "çŠ¶æ€": note
                            })
                            stats["hit"] += 1
            
        except Exception as e:
            st.error(f"ç¬¬ {page} é¡µé”™è¯¯: {e}")
            
    progress_bar.progress(100)
    status_text.success(f"âœ… æŠ“å–å®Œæˆï¼å…±æ£€æŸ¥ {stats['processed']} ç¯‡ï¼Œç¬¦åˆæ¡ä»¶ {stats['hit']} ç¯‡ã€‚")
    log_area.empty()
    return results

# --- Streamlit ç•Œé¢ ---

st.set_page_config(page_title="Gooood ç»ˆæç­›é€‰ç‰ˆ", layout="wide", page_icon="ğŸ›ï¸")

st.title("ğŸ›ï¸ Gooood.cn æ¡ˆä¾‹ç­›é€‰ (ç»ˆæä¿®å¤ç‰ˆ)")
st.markdown("""
**V3.0 æ›´æ–°**ï¼š
1. **æƒå¨è®¡æ•°**ï¼šä¼˜å…ˆè¯»å–è¯¦æƒ…é¡µçš„â€œXX è¯„è®ºâ€å¤§æ ‡é¢˜ï¼Œç¡®ä¿ä¸é”™è¿‡ã€‚
2. **å…œåº•ç­–ç•¥**ï¼šå¦‚æœèƒ½çœ‹åˆ°è¯„è®ºæ•°ä½†æŠ“ä¸åˆ°ä½œè€…ï¼ˆæ— æ³•åˆ¤æ–­æ˜¯å¦ä¸º falseï¼‰ï¼Œé»˜è®¤ä¿ç•™ï¼Œé˜²æ­¢è¯¯åˆ ã€‚
3. **å¼ºåˆ¶æ£€æŸ¥**ï¼šå¯¹æ‰€æœ‰ç¬¦åˆæ ‡é¢˜è§„èŒƒçš„æ–‡ç« ï¼Œé€ä¸€è¿›å…¥è¯¦æƒ…é¡µæ£€æŸ¥ã€‚
""")

with st.sidebar:
    st.header("ğŸ› ï¸ å‚æ•°è®¾ç½®")
    col_p1, col_p2 = st.columns(2)
    start_p = col_p1.number_input("èµ·å§‹é¡µç ", min_value=1, value=800, step=1)
    end_p = col_p2.number_input("ç»“æŸé¡µç ", min_value=1, value=805, step=1)
    
    st.markdown("---")
    min_c = st.number_input("æœ€å°è¯„è®ºæ•° (N)", min_value=0, value=1)
    target_user = st.text_input("å‰”é™¤å•ä¸€è¯„è®ºç”¨æˆ·", value="false")
    st.markdown("---")
    run_btn = st.button("ğŸš€ å¼€å§‹ä¸¥æ ¼æŠ“å–", type="primary", use_container_width=True)

tab1, tab2 = st.tabs(["ğŸ“‹ ç»“æœåˆ—è¡¨", "ğŸ•’ å†å²è®°å½•"])

with tab1:
    if run_btn:
        if start_p > end_p:
            st.error("âŒ èµ·å§‹é¡µç ä¸èƒ½å¤§äºç»“æŸé¡µç ")
        else:
            with st.spinner(f'æ­£åœ¨å¯¹ç¬¬ {start_p}-{end_p} é¡µè¿›è¡Œå…¨é‡æ£€æŸ¥...'):
                data = scrape_logic_strict(start_p, end_p, min_c, target_user)
            
            if data:
                df = pd.DataFrame(data)
                save_history({
                    "criteria": f"Page: {start_p}-{end_p} | Min: {min_c}",
                    "count": len(data),
                    "data": data
                })
                
                st.success(f"âœ… æ‰¾åˆ° {len(data)} ä¸ªæ¡ˆä¾‹ï¼")
                st.data_editor(
                    df,
                    column_config={
                        "é“¾æ¥": st.column_config.LinkColumn("ç‚¹å‡»æŸ¥çœ‹"),
                        "è¯„è®ºæ•°": st.column_config.NumberColumn("çƒ­åº¦", format="%d ğŸ’¬"),
                    },
                    hide_index=True,
                    use_container_width=True
                )
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (CSV)", csv, "gooood_v3_results.csv", "text/csv")
            else:
                st.warning("âš ï¸ åœ¨æŒ‡å®šèŒƒå›´å†…æœªæ‰¾åˆ°æ»¡è¶³è¯„è®ºæ•°è¦æ±‚çš„æ¡ˆä¾‹ã€‚")

with tab2:
    st.header("å†å²è®°å½•")
    history_data = load_history()
    if not history_data:
        st.caption("æš‚æ— å†å²è®°å½•")
    else:
        for i, record in enumerate(history_data):
            with st.expander(f"ğŸ“… {record['saved_at']} - {record.get('criteria','')} (ç»“æœ: {record['count']})"):
                if record['data']:
                    st.dataframe(pd.DataFrame(record['data']), hide_index=True)
