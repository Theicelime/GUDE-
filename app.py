import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import os
import json
import re

# --- é…ç½®éƒ¨åˆ† ---
HISTORY_FILE = "search_history.json"
BASE_URL = "https://www.gooood.cn"

# éšæœº User-Agent æ± ï¼Œé˜²æ­¢è¢«å½“ä½œå•ä¸€çˆ¬è™«å±è”½
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36"
]

HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Connection": "keep-alive",
    "Referer": "https://www.gooood.cn/"
}

# --- è¾…åŠ©å‡½æ•° ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(record):
    history = load_history()
    record['saved_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    history.insert(0, record)
    if len(history) > 20: 
        history = history[:20]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def has_brackets(title):
    if not title: return False
    return (re.search(r'ï¼ˆ[^ï¼‰]*ï¼‰', title) is not None or 
            re.search(r'\([^)]*\)', title) is not None)

def contains_chinese(text):
    if not text: return False
    return bool(re.search(r'[\u4e00-\u9fff]+', text))

def scan_html_for_count(html_text):
    """
    [æ ¸å¼¹çº§] æš´åŠ›æ–‡æœ¬æœç´¢
    ä¸è§£æ DOMï¼Œç›´æ¥åœ¨ HTML æºç å­—ç¬¦ä¸²é‡Œæ‰¾ "11 è¯„è®º" è¿™ç§æ¨¡å¼
    """
    count = 0
    # æ¨¡å¼1: "11 è¯„è®º" æˆ– "11 æ¡è¯„è®º" æˆ– "11 Comments"
    # [>\"\s] ç¡®ä¿æ•°å­—å‰é¢æ˜¯æ ‡ç­¾ç»“æŸç¬¦ > æˆ–ç©ºæ ¼æˆ–å¼•å·ï¼Œé˜²æ­¢æŠŠ ID é‡Œçš„æ•°å­—è¯»å‡ºæ¥
    matches = re.findall(r'[>\"\s](\d+)\s*(?:æ¡)?(?:è¯„è®º|Comments)', html_text, re.IGNORECASE)
    if matches:
        # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„æ•°å­—ï¼Œå–æœ€å¤§çš„é‚£ä¸ªï¼ˆé˜²æ­¢æŠ“åˆ°ä¾§è¾¹æ çš„çƒ­é—¨è¯„è®ºæ•°ï¼‰
        # è¿‡æ»¤æ‰è¿‡å¤§çš„ç¦»è°±æ•°å­—ï¼ˆæ¯”å¦‚å¹´ä»½ 2020ï¼‰
        valid_nums = [int(m) for m in matches if int(m) < 1000]
        if valid_nums:
            count = max(valid_nums)
            
    # æ¨¡å¼2: "è¯„è®º (11)"
    if count == 0:
        matches2 = re.findall(r'(?:è¯„è®º|Comments)\s*[:\uff1a\(ï¼ˆ]\s*(\d+)', html_text, re.IGNORECASE)
        if matches2:
            valid_nums = [int(m) for m in matches2 if int(m) < 1000]
            if valid_nums:
                count = max(valid_nums)
    
    return count

def extract_authors(soup):
    """æå–ä½œè€…åˆ—è¡¨"""
    authors = []
    # ç©·ä¸¾æ‰€æœ‰å¯èƒ½çš„ä½œè€…æ ‡ç­¾ class
    selectors = [
        '.fn', '.comment-author', 'cite', '.url', 
        '.comment-meta .author', '.comment-body b', 
        '.vcard .fn', 'a[rel="external nofollow"]'
    ]
    for sel in selectors:
        tags = soup.select(sel)
        for t in tags:
            name = t.get_text(strip=True)
            # è¿‡æ»¤æ‰ä¸€äº›ä¸æ˜¯äººåçš„å…³é”®è¯
            if name and len(name) < 30 and "å›å¤" not in name and "20" not in name:
                authors.append(name)
    return authors

def fetch_detail_and_count(article_url, target_user="false"):
    """
    [V4.0 é€»è¾‘]
    1. è¯·æ±‚ç½‘é¡µ
    2. æš´åŠ›æ­£åˆ™æœç´¢è¯„è®ºæ•°
    3. è§£æ DOM æœç´¢ä½œè€…å
    4. ç»¼åˆåˆ¤æ–­
    """
    try:
        time.sleep(random.uniform(0.5, 1.5))
        
        # éšæœºåˆ‡æ¢ UA
        current_headers = HEADERS.copy()
        current_headers["User-Agent"] = random.choice(USER_AGENTS)
        
        resp = requests.get(article_url, headers=current_headers, timeout=20)
        resp.encoding = 'utf-8' # å¼ºåˆ¶ UTF-8ï¼Œé˜²æ­¢ä¹±ç å¯¼è‡´æ­£åˆ™å¤±è´¥
        
        if resp.status_code != 200:
            return None, 0, f"HTTP {resp.status_code}"
        
        html_text = resp.text
        soup = BeautifulSoup(html_text, 'html.parser')
        
        # 1. æš´åŠ›æœç´¢è¯„è®ºæ•°
        regex_count = scan_html_for_count(html_text)
        
        # 2. å°è¯•æå–ä½œè€…
        authors = extract_authors(soup)
        real_author_count = len(authors)
        
        # 3. æœ€ç»ˆæ•°é‡å–æœ€å¤§å€¼
        final_count = max(regex_count, real_author_count)
        
        if final_count == 0:
            return True, 0, "æ— è¯„è®º"

        # 4. è¿‡æ»¤é€»è¾‘
        # åªæœ‰å½“æˆ‘ä»¬æ—¢æŠ“åˆ°äº†æ•°é‡ï¼ŒåˆæŠ“åˆ°äº†ä½œè€…åæ—¶ï¼Œæ‰èƒ½è¿›è¡Œ false è¿‡æ»¤
        if len(authors) > 0:
            unique_authors = set(authors)
            target_user_lower = target_user.lower()
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä½œè€…éƒ½æ˜¯ target_user
            all_match = True
            for u in unique_authors:
                if target_user_lower not in u.lower():
                    all_match = False
                    break
            
            if all_match:
                return False, final_count, f"ä»…å«ç”¨æˆ· {target_user}"
            else:
                return True, final_count, "æœ‰æ•ˆ (å·²éªŒè¯ä½œè€…)"
        
        # å¦‚æœæ­£åˆ™æŠ“åˆ°äº†æ•°é‡(ä¾‹å¦‚11)ï¼Œä½†æ²¡æŠ“åˆ°ä½œè€…å(HTMLç»“æ„å¤ªæ€ª)
        # æ­¤æ—¶æ— æ³•è¿‡æ»¤ falseï¼Œä½†ä¸ºäº†ä¸æ¼æ‰ï¼Œæˆ‘ä»¬å¿…é¡»ä¿ç•™ï¼
        if regex_count > 0:
            return True, regex_count, f"æ£€æµ‹åˆ° {regex_count} æ¡è¯„è®º (ä½œè€…æœªçŸ¥)"

        return True, final_count, "æœ‰æ•ˆ"

    except Exception as e:
        return None, 0, f"Err: {str(e)[:20]}"

def scrape_logic_v4(start_page, end_page, min_comments, target_user_filter):
    results = []
    
    status_text = st.empty()
    progress_bar = st.progress(0)
    log_area = st.empty()
    
    total_pages = end_page - start_page + 1
    stats = {"processed": 0, "hit": 0}
    
    for i, page in enumerate(range(start_page, end_page + 1)):
        progress_percentage = (i) / total_pages
        progress_bar.progress(progress_percentage)
        
        url = f"{BASE_URL}/page/{page}" if page > 1 else BASE_URL
        status_text.markdown(f"**âš¡ æ­£åœ¨æš´åŠ›æ‰«æç¬¬ {page} é¡µ...** (å‘½ä¸­: {stats['hit']})")
        
        try:
            resp = requests.get(url, headers={"User-Agent": random.choice(USER_AGENTS)}, timeout=20)
            if resp.status_code != 200:
                continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            # å…¼å®¹å¤šç§æ–‡ç« å®¹å™¨
            articles = soup.select('.post') or soup.select('article') or soup.select('.type-post')
            
            if not articles:
                # å¦‚æœè¿æ–‡ç« åˆ—è¡¨éƒ½æŠ“ä¸åˆ°ï¼Œå¯èƒ½æ˜¯åçˆ¬æˆ–è€…ç»“æ„å˜äº†ï¼Œå°è¯•ç›´æ¥æ‰¾ h2 a
                links = soup.select('h2 a')
                if not links:
                    st.warning(f"ç¬¬ {page} é¡µæœªè¯†åˆ«åˆ°æ–‡ç« åˆ—è¡¨")
                    continue
                # æ„é€ ä¸´æ—¶ article å¯¹è±¡
                articles = [{'link': l['href'], 'title': l.get_text(strip=True)} for l in links]
            else:
                # æå–æ ‡å‡†ç»“æ„
                temp_articles = []
                for art in articles:
                    t_tag = art.select_one('h2 a') or art.select_one('h1 a') or art.select_one('a[rel="bookmark"]')
                    if t_tag:
                        temp_articles.append({'link': t_tag['href'], 'title': t_tag.get_text(strip=True)})
                articles = temp_articles

            for art in articles:
                title = art['title']
                link = art['link']
                
                # æ¸…æ´—
                if not contains_chinese(title): continue
                if has_brackets(title): continue
                
                stats["processed"] += 1
                log_area.text(f"æ­£åœ¨æ£€æŸ¥: {title[:25]}...")
                
                # è¿›å…¥è¯¦æƒ…é¡µ
                is_valid, count, note = fetch_detail_and_count(link, target_user_filter)
                
                if is_valid is True:
                    if count >= min_comments:
                        results.append({
                            "é¡µç ": page,
                            "æ ‡é¢˜": title,
                            "é“¾æ¥": link,
                            "è¯„è®ºæ•°": count,
                            "çŠ¶æ€": note
                        })
                        stats["hit"] += 1
            
        except Exception as e:
            st.error(f"Page {page} error: {e}")
            
    progress_bar.progress(100)
    status_text.success(f"å®Œæˆï¼æ£€æŸ¥ {stats['processed']} ç¯‡ï¼Œå‘½ä¸­ {stats['hit']} ç¯‡ã€‚")
    log_area.empty()
    return results

# --- Streamlit ç•Œé¢ ---

st.set_page_config(page_title="Gooood æš´åŠ›æŠ“å–ç‰ˆ", layout="wide", page_icon="âš¡")

st.title("âš¡ Gooood.cn æš´åŠ›æŠ“å–å·¥å…· (V4.0)")
st.markdown("""
**æ ¸å¿ƒé€»è¾‘**ï¼š
1. **æš´åŠ›æ­£åˆ™**ï¼šç›´æ¥åœ¨ç½‘é¡µæºä»£ç ä¸­æœç´¢ `11 è¯„è®º` æˆ– `11 Comments` å­—æ ·ï¼Œä¸ä¾èµ– HTML æ ‡ç­¾ã€‚
2. **å¼ºåˆ¶æ£€æŸ¥**ï¼šå¯¹æ‰€æœ‰æ ‡é¢˜åˆè§„çš„æ–‡ç« ï¼Œé€ä¸€è¿›å…¥è¯¦æƒ…é¡µã€‚
3. **å®‰å…¨å…œåº•**ï¼šå¦‚æœæ£€æµ‹åˆ°è¯„è®ºæ•°ä½†æŠ“ä¸åˆ°ä½œè€…åï¼Œå¼ºåˆ¶ä¿ç•™ï¼Œé˜²æ­¢è¯¯åˆ ã€‚
""")

with st.sidebar:
    st.header("ğŸ› ï¸ å‚æ•°è®¾ç½®")
    col_p1, col_p2 = st.columns(2)
    start_p = col_p1.number_input("èµ·å§‹é¡µç ", min_value=1, value=800, step=1)
    end_p = col_p2.number_input("ç»“æŸé¡µç ", min_value=1, value=805, step=1)
    
    min_c = st.number_input("æœ€å°è¯„è®ºæ•° (N)", min_value=0, value=1)
    target_user = st.text_input("å‰”é™¤å•ä¸€è¯„è®ºç”¨æˆ·", value="false")
    run_btn = st.button("ğŸš€ å¼€å§‹æš´åŠ›æŠ“å–", type="primary", use_container_width=True)

tab1, tab2 = st.tabs(["ğŸ“‹ ç»“æœåˆ—è¡¨", "ğŸ•’ å†å²è®°å½•"])

with tab1:
    if run_btn:
        with st.spinner(f'æ­£åœ¨å¯¹ç¬¬ {start_p}-{end_p} é¡µè¿›è¡Œæš´åŠ›æ‰«æ...'):
            data = scrape_logic_v4(start_p, end_p, min_c, target_user)
        
        if data:
            df = pd.DataFrame(data)
            save_history({"criteria": f"Page: {start_p}-{end_p}", "count": len(data), "data": data})
            st.success(f"âœ… æ‰¾åˆ° {len(data)} ä¸ªæ¡ˆä¾‹ï¼")
            st.data_editor(
                df,
                column_config={
                    "é“¾æ¥": st.column_config.LinkColumn("ç‚¹å‡»æŸ¥çœ‹"),
                    "è¯„è®ºæ•°": st.column_config.NumberColumn("çƒ­åº¦", format="%d ğŸ’¬"),
                },
                hide_index=True,
                use_container_width=True
            )
        else:
            st.warning("âš ï¸ æœªæ‰¾åˆ°æ¡ˆä¾‹ã€‚å¦‚æœè¿™è¿˜æ‰¾ä¸åˆ°ï¼Œè¯´æ˜è¯¥ç½‘ç«™å¯èƒ½å¯ç”¨äº†é«˜çº§åçˆ¬æˆ–è¯„è®ºæ˜¯çº¯åŠ¨æ€åŠ è½½çš„ã€‚")

with tab2:
    st.header("å†å²è®°å½•")
    h_data = load_history()
    if h_data:
        for i, rec in enumerate(h_data):
            with st.expander(f"{rec['saved_at']} (ç»“æœ: {rec['count']})"):
                if rec['data']: st.dataframe(pd.DataFrame(rec['data']), hide_index=True)
