import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import os
import json
import re

# --- é…ç½®éƒ¨åˆ† ---
HISTORY_FILE = "search_history.json"
BASE_URL = "https://www.gooood.cn"

# éšæœº User-Agent
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
]

# --- è¾…åŠ©å‡½æ•° ---

def load_history():
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(record):
    history = load_history()
    record['saved_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    history.insert(0, record)
    if len(history) > 20: 
        history = history[:20]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def has_brackets(title):
    if not title: return False
    return (re.search(r'ï¼ˆ[^ï¼‰]*ï¼‰', title) is not None or 
            re.search(r'\([^)]*\)', title) is not None)

def contains_chinese(text):
    if not text: return False
    return bool(re.search(r'[\u4e00-\u9fff]+', text))

def get_real_comment_count(soup, html_text):
    """
    [V5.0 æ ¸å¿ƒ] æ··åˆè®¡ç®—ç­–ç•¥
    """
    # ç­–ç•¥ 1: æ•°å…ƒç´  (æœ€å‡†)
    # WordPress é€šå¸¸ä½¿ç”¨ ol.commentlist > li.comment æˆ– .comment-list > li
    # æˆ–è€…ç›´æ¥æ•° .comment-body çš„æ•°é‡
    count_by_tag = 0
    
    # æŸ¥æ‰¾è¯„è®ºåˆ—è¡¨å®¹å™¨
    comment_list = soup.select_one('.commentlist') or soup.select_one('.comment-list')
    if comment_list:
        # ç›´æ¥æ•°åˆ—è¡¨ä¸‹çš„ li æ•°é‡
        count_by_tag = len(comment_list.find_all('li', recursive=False))
    
    if count_by_tag == 0:
        # å¦‚æœæ²¡æ‰¾åˆ°åˆ—è¡¨å®¹å™¨ï¼Œç›´æ¥æ•°æ‰€æœ‰ class åŒ…å« comment-body çš„å…ƒç´ 
        count_by_tag = len(soup.select('.comment-body'))
    
    # ç­–ç•¥ 2: æ­£åˆ™åŒ¹é…æ–‡æœ¬ (å…œåº•)
    # åŒ¹é… "11è¯„è®º", "11 è¯„è®º", "11 æ¡è¯„è®º", "Comments (11)", "è¯„è®ºï¼š11"
    count_by_text = 0
    patterns = [
        r'(\d+)\s*(?:æ¡)?\s*(?:è¯„è®º|Comments?)',  # 11 è¯„è®º
        r'(?:è¯„è®º|Comments?)\s*[:\uff1a\(ï¼ˆ]\s*(\d+)'  # è¯„è®º: 11
    ]
    
    for p in patterns:
        matches = re.findall(p, html_text, re.IGNORECASE)
        if matches:
            # è¿‡æ»¤æ‰å¹´ä»½ç­‰å¹²æ‰°é¡¹(å‡è®¾è¯„è®ºæ•°ä¸ä¼šè¶…è¿‡2000)
            valid = [int(m) for m in matches if int(m) < 2000]
            if valid:
                count_by_text = max(valid)
                break
    
    # è¿”å›ä¸¤è€…ä¸­è¾ƒå¤§çš„é‚£ä¸ª
    return max(count_by_tag, count_by_text), count_by_tag, count_by_text

def get_authors(soup):
    """æå–æ‰€æœ‰è¯„è®ºè€…åå­—"""
    authors = []
    # ç©·ä¸¾ä½œè€…æ ‡ç­¾
    selectors = ['.fn', '.comment-author', '.url', 'cite', '.vcard']
    for sel in selectors:
        for tag in soup.select(sel):
            name = tag.get_text(strip=True)
            if name and len(name) < 50: # æ’é™¤è¿‡é•¿çš„é”™è¯¯æ–‡æœ¬
                authors.append(name)
    return authors

def process_detail_page(url, target_user, debug=False):
    """å¤„ç†è¯¦æƒ…é¡µ"""
    try:
        time.sleep(random.uniform(0.5, 1.0))
        resp = requests.get(url, headers={"User-Agent": random.choice(USER_AGENTS)}, timeout=15)
        resp.encoding = 'utf-8'
        
        if resp.status_code != 200:
            return None, 0, f"HTTP {resp.status_code}"
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # 1. è·å–æ•°é‡
        final_count, count_tag, count_text = get_real_comment_count(soup, resp.text)
        
        if debug:
            st.text(f"[DEBUG] {url}\n -> æ ‡ç­¾ç»Ÿè®¡: {count_tag}, æ–‡æœ¬æ­£åˆ™: {count_text} -> æœ€ç»ˆè®¤å®š: {final_count}")

        if final_count == 0:
            return True, 0, "æ— è¯„è®º"
            
        # 2. è·å–ä½œè€…å¹¶è¿‡æ»¤
        authors = get_authors(soup)
        
        # å¦‚æœæœ‰ä½œè€…ï¼Œè¿›è¡Œ target_user è¿‡æ»¤
        if len(authors) > 0:
            unique = set([a.lower() for a in authors])
            target_lower = target_user.lower()
            
            # å¦‚æœå»é‡ååªæœ‰è¿™ä¸€ä¸ªç”¨æˆ·
            if len(unique) == 1 and target_lower in unique:
                return False, final_count, f"ä»…å«ç”¨æˆ· {target_user}"
            
            return True, final_count, "æœ‰æ•ˆ"
            
        # å¦‚æœæ²¡æŠ“åˆ°ä½œè€…ï¼Œä½†æœ‰æ•°é‡ -> ä¿ç•™
        return True, final_count, "æœ‰æ•ˆ (æœ‰æ•°é‡æ— ä½œè€…è¯¦æƒ…)"

    except Exception as e:
        return None, 0, f"Error: {str(e)}"

def run_scraper(start_p, end_p, min_c, target_u, debug_mode):
    results = []
    
    status_box = st.empty()
    bar = st.progress(0)
    
    total = end_p - start_p + 1
    stats = {"checked": 0, "found": 0}
    
    for i, page in enumerate(range(start_p, end_p + 1)):
        bar.progress(i / total)
        status_box.markdown(f"**æ­£åœ¨æ‰«æç¬¬ {page} é¡µ...** (å·²æ‰¾åˆ° {stats['found']} ä¸ª)")
        
        url = f"{BASE_URL}/page/{page}" if page > 1 else BASE_URL
        
        try:
            r = requests.get(url, headers={"User-Agent": random.choice(USER_AGENTS)}, timeout=15)
            if r.status_code != 200: continue
            
            soup = BeautifulSoup(r.text, 'html.parser')
            # å…¼å®¹å¤šç§ç»“æ„
            posts = soup.select('.post') or soup.select('article')
            
            if not posts:
                # å°è¯•ç›´æ¥æ‰¾é“¾æ¥
                if debug_mode: st.warning(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ° .post å…ƒç´ ï¼Œå°è¯•ç›´æ¥æœç´¢é“¾æ¥")
                h2_links = soup.select('h2 a')
                posts = [{'link': a['href'], 'title': a.get_text(strip=True)} for a in h2_links]
            else:
                # æå–æ ‡å‡†ç»“æ„
                temp = []
                for p in posts:
                    a = p.select_one('h2 a') or p.select_one('h1 a')
                    if a: temp.append({'link': a['href'], 'title': a.get_text(strip=True)})
                posts = temp
                
            for post in posts:
                title = post['title']
                link = post['link']
                
                # æ ‡é¢˜æ¸…æ´—
                if not contains_chinese(title): continue
                if has_brackets(title): continue
                
                stats["checked"] += 1
                
                # è¯¦æƒ…é¡µæ£€æŸ¥
                is_valid, count, note = process_detail_page(link, target_u, debug_mode)
                
                if is_valid is True:
                    if count >= min_c:
                        results.append({
                            "é¡µç ": page,
                            "æ ‡é¢˜": title,
                            "è¯„è®ºæ•°": count,
                            "çŠ¶æ€": note,
                            "é“¾æ¥": link
                        })
                        stats["found"] += 1
                        
        except Exception as e:
            if debug_mode: st.error(f"Page {page} error: {e}")
            
    bar.progress(100)
    status_box.success(f"å®Œæˆï¼å…±æ‰«æ {stats['checked']} ç¯‡ï¼Œç¬¦åˆæ¡ä»¶ {stats['found']} ç¯‡ã€‚")
    return results

# --- UI ---

st.set_page_config(page_title="Gooood V5.0", layout="wide")
st.title("ğŸ›ï¸ Gooood æ¡ˆä¾‹ç­›é€‰ (V5.0 ç»ˆææ•°äººå¤´ç‰ˆ)")

with st.sidebar:
    st.header("è®¾ç½®")
    c1, c2 = st.columns(2)
    s_p = c1.number_input("èµ·å§‹é¡µ", value=800, step=1)
    e_p = c2.number_input("ç»“æŸé¡µ", value=805, step=1)
    min_c = st.number_input("æœ€å°è¯„è®ºæ•°", value=1, min_value=0)
    t_u = st.text_input("æ’é™¤å•ä¸€ç”¨æˆ·", value="false")
    
    st.markdown("---")
    debug = st.checkbox("å¼€å¯è°ƒè¯•æ¨¡å¼ (æ˜¾ç¤ºè¯¦ç»†æŠ“å–è¿‡ç¨‹)", value=False)
    btn = st.button("å¼€å§‹æŠ“å–", type="primary", use_container_width=True)

tab1, tab2 = st.tabs(["ç»“æœ", "å†å²"])

with tab1:
    if btn:
        if s_p > e_p:
            st.error("é¡µç é”™è¯¯")
        else:
            with st.spinner("æ­£åœ¨æŠ“å–..."):
                data = run_scraper(s_p, e_p, min_c, t_u, debug)
                
            if data:
                df = pd.DataFrame(data)
                save_history({"criteria": f"P{s_p}-{e_p}", "count": len(data), "data": data})
                st.success(f"æ‰¾åˆ° {len(data)} æ¡æ•°æ®")
                st.data_editor(
                    df,
                    column_config={
                        "é“¾æ¥": st.column_config.LinkColumn(),
                        "è¯„è®ºæ•°": st.column_config.NumberColumn(format="%d ğŸ’¬")
                    },
                    use_container_width=True,
                    hide_index=True
                )
            else:
                st.warning("æœªæ‰¾åˆ°æ•°æ®ã€‚è¯·å°è¯•å‹¾é€‰'å¼€å¯è°ƒè¯•æ¨¡å¼'æŸ¥çœ‹å…·ä½“åŸå› ã€‚")

with tab2:
    st.header("å†å²")
    h = load_history()
    if h:
        for r in h:
            with st.expander(f"{r['saved_at']} ({r['count']}æ¡)"):
                st.dataframe(pd.DataFrame(r['data']), hide_index=True)
