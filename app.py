import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import os
import json
import re

# --- é…ç½®éƒ¨åˆ† ---
HISTORY_FILE = "search_history.json"
BASE_URL = "https://www.gooood.cn"

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´ (æ¨¡æ‹Ÿ Chrome 120)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Connection": "keep-alive"
}

# --- è¾…åŠ©å‡½æ•° ---

def load_history():
    """åŠ è½½å†å²è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(record):
    """ä¿å­˜å†å²è®°å½•"""
    history = load_history()
    record['saved_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    history.insert(0, record)
    if len(history) > 20: 
        history = history[:20]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def has_brackets(title):
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«æ‹¬å· (æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡)"""
    if not title: return False
    # ä¸¥æ ¼æŒ‰ç…§ä½ ç»™çš„å‚è€ƒä»£ç é€»è¾‘ï¼šæœ‰ä»»ä½•ä¸€ç§æ‹¬å·å°±è¿‡æ»¤
    return (re.search(r'ï¼ˆ[^ï¼‰]*ï¼‰', title) is not None or 
            re.search(r'\([^)]*\)', title) is not None)

def contains_chinese(text):
    """æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    if not text: return False
    return bool(re.search(r'[\u4e00-\u9fff]+', text))

def fetch_detail_and_count(article_url, target_user="false"):
    """
    [ä¸¥æ ¼æ‰§è¡Œ] è¿›å…¥è¯¦æƒ…é¡µè·å–çœŸå®æ•°æ®
    ä¸ç®¡åˆ—è¡¨é¡µè¯´ä»€ä¹ˆï¼Œéƒ½ä»¥è¿™é‡ŒæŠ“åˆ°çš„ä¸ºå‡†ã€‚
    """
    try:
        # å¿…é¡»æœ‰å»¶æ—¶ï¼Œå¦åˆ™è¿ç»­è¯·æ±‚ä¼šè¢«å°
        time.sleep(random.uniform(0.5, 1.2))
        
        resp = requests.get(article_url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            return None, 0, f"Error {resp.status_code}"
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # --- æŠ“å–è¯„è®ºè€…é€»è¾‘ ---
        # å¯»æ‰¾è¯„è®ºä¸»ä½“
        comment_elements = soup.select('.comment-body')
        if not comment_elements:
            # å¤‡ç”¨ï¼šæœ‰äº›è€é¡µé¢ç»“æ„ä¸åŒ
            comment_elements = soup.select('li.comment')
            
        authors = []
        for c in comment_elements:
            # å°è¯•è·å–ä½œè€…åï¼Œå…¼å®¹å¤šç§ class
            author_tag = c.select_one('.fn') or c.select_one('.comment-author') or c.select_one('.url')
            if author_tag:
                authors.append(author_tag.get_text(strip=True))
        
        real_count = len(authors)
        
        # --- è¿‡æ»¤é€»è¾‘ ---
        if real_count == 0:
            return True, 0, "æ— è¯„è®º" # æ ‡è®°ä¸ºæœ‰æ•ˆè®¿é—®ï¼Œä½†æ•°é‡ä¸º0

        # æ£€æŸ¥æ˜¯å¦åªæœ‰ target_user
        unique_authors = set(authors)
        # è½¬æ¢ä¸ºå°å†™æ¯”è¾ƒ
        target_user_lower = target_user.lower()
        unique_lower = {u.lower() for u in unique_authors}
        
        if len(unique_lower) == 1 and target_user_lower in unique_lower:
            return False, real_count, f"ä»…å«ç”¨æˆ· {target_user}"
            
        return True, real_count, "æœ‰æ•ˆ"

    except Exception as e:
        return None, 0, f"è§£æå¼‚å¸¸: {str(e)}"

def scrape_logic_strict(start_page, end_page, min_comments, target_user_filter):
    """
    [ä¸¥æ ¼æ¨¡å¼çˆ¬è™«]
    é€»è¾‘ï¼šéå†åˆ—è¡¨ -> æ ‡é¢˜æ¸…æ´— -> å¼ºåˆ¶è¿›è¯¦æƒ…é¡µ -> ç»Ÿè®¡ç­›é€‰
    """
    results = []
    
    # ç•Œé¢å…ƒç´ 
    status_text = st.empty()
    progress_bar = st.progress(0)
    log_area = st.empty() # ç”¨äºæ˜¾ç¤ºå®æ—¶å¤„ç†çš„æ ‡é¢˜
    
    total_pages = end_page - start_page + 1
    
    # ç»Ÿè®¡
    stats = {"processed": 0, "hit": 0}
    
    for i, page in enumerate(range(start_page, end_page + 1)):
        # æ›´æ–°æ€»è¿›åº¦
        progress_percentage = (i) / total_pages
        progress_bar.progress(progress_percentage)
        
        url = f"{BASE_URL}/page/{page}" if page > 1 else BASE_URL
        status_text.markdown(f"**ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...** (å·²å‘½ä¸­: {stats['hit']} ä¸ª)")
        
        try:
            # 1. è·å–åˆ—è¡¨é¡µ
            resp = requests.get(url, headers=HEADERS, timeout=20)
            if resp.status_code != 200:
                st.warning(f"âš ï¸ ç¬¬ {page} é¡µè®¿é—®å¤±è´¥ (Code: {resp.status_code})")
                continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            articles = soup.select('.post') 
            
            if not articles:
                st.info(f"ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°æ–‡ç« ï¼Œåœæ­¢ã€‚")
                break

            # 2. éå†è¯¥é¡µçš„æ‰€æœ‰æ–‡ç« 
            page_articles = []
            # å…ˆæŠŠè¿™ä¸€é¡µçš„æ ‡é¢˜å’Œé“¾æ¥éƒ½æå–å‡ºæ¥ï¼Œé¿å…åœ¨å¾ªç¯é‡Œæ“ä½œ soup å¯¹è±¡å‡ºé”™
            for art in articles:
                t_tag = art.select_one('h2 a') or art.select_one('h1 a') or art.select_one('.entry-title a')
                if t_tag:
                    title = t_tag.get_text(strip=True)
                    link = t_tag['href']
                    page_articles.append((title, link))
            
            # 3. å¯¹æå–å‡ºçš„æ–‡ç« é€ä¸ªè¿›è¡Œè¯¦æƒ…é¡µæ£€æŸ¥
            for idx, (title, link) in enumerate(page_articles):
                
                # --- A. æ ‡é¢˜æ¸…æ´— (æœ¬åœ°å¿«é€Ÿè¿‡æ»¤) ---
                if not contains_chinese(title): 
                    continue # æ— ä¸­æ–‡ï¼Œè·³è¿‡
                if has_brackets(title): 
                    continue # æœ‰æ‹¬å·ï¼Œè·³è¿‡
                
                # --- B. å¼ºåˆ¶è¿›å…¥è¯¦æƒ…é¡µ (ç½‘ç»œè¯·æ±‚) ---
                stats["processed"] += 1
                log_area.text(f"æ­£åœ¨æ£€æŸ¥ [{stats['processed']}] : {title[:30]}...")
                
                is_valid_user, real_count, note = fetch_detail_and_count(link, target_user_filter)
                
                # --- C. ç»“æœåˆ¤æ–­ ---
                # is_valid_user=None è¡¨ç¤ºè¯·æ±‚æŠ¥é”™
                # is_valid_user=False è¡¨ç¤ºåªæœ‰ false ç”¨æˆ·
                # is_valid_user=True è¡¨ç¤ºç”¨æˆ·æ£€æŸ¥é€šè¿‡
                
                if is_valid_user is True:
                    if real_count >= min_comments:
                        # ç¬¦åˆæ¡ä»¶ï¼
                        results.append({
                            "é¡µç ": page,
                            "æ ‡é¢˜": title,
                            "é“¾æ¥": link,
                            "è¯„è®ºæ•°": real_count,
                            "çŠ¶æ€": note
                        })
                        stats["hit"] += 1
            
        except Exception as e:
            st.error(f"ç¬¬ {page} é¡µå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            
    progress_bar.progress(100)
    status_text.success(f"âœ… æŠ“å–å®Œæˆï¼å…±æ£€æŸ¥ {stats['processed']} ç¯‡ï¼Œç¬¦åˆæ¡ä»¶ {stats['hit']} ç¯‡ã€‚")
    log_area.empty()
    return results

# --- Streamlit ç•Œé¢ ---

st.set_page_config(page_title="Gooood ä¸¥æ ¼ç­›é€‰å·¥å…·", layout="wide", page_icon="ğŸ›ï¸")

st.title("ğŸ›ï¸ Gooood.cn æ¡ˆä¾‹ç­›é€‰ (ä¸¥æ ¼æ¨¡å¼)")
st.markdown("""
**ä¸¥æ ¼æ¨¡å¼é€»è¾‘**ï¼š
1. **æ ‡é¢˜æ¸…æ´—**ï¼šä¿ç•™å«ä¸­æ–‡ä¸”ä¸å«æ‹¬å·çš„æ ‡é¢˜ã€‚
2. **å¼ºåˆ¶æ£€æŸ¥**ï¼šå¯¹æ‰€æœ‰æ¸…æ´—åçš„æ ‡é¢˜ï¼Œ**é€ä¸€è¿›å…¥è¯¦æƒ…é¡µ**ç»Ÿè®¡çœŸå®è¯„è®ºæ•°ã€‚
3. **ç²¾å‡†è¿‡æ»¤**ï¼šå‰”é™¤ `false` ç”¨æˆ·ï¼Œä¿ç•™è¯„è®ºæ•° >= N çš„æ¡ˆä¾‹ã€‚
""")

with st.sidebar:
    st.header("ğŸ› ï¸ å‚æ•°è®¾ç½®")
    col_p1, col_p2 = st.columns(2)
    start_p = col_p1.number_input("èµ·å§‹é¡µç ", min_value=1, value=100, step=1)
    end_p = col_p2.number_input("ç»“æŸé¡µç ", min_value=1, value=102, step=1)
    
    st.caption("âš ï¸ æ³¨æ„ï¼šå› ä¸ºè¦è¿›å…¥æ¯ä¸ªè¯¦æƒ…é¡µï¼Œé€Ÿåº¦ä¼šæ¯”ç²—ç•¥æ‰«ææ…¢ï¼Œä½†æ•°æ®ç»å¯¹å‡†ç¡®ã€‚å»ºè®®ä¸€æ¬¡æ‰«æ 5-10 é¡µã€‚")
    
    st.markdown("---")
    min_c = st.number_input("æœ€å°è¯„è®ºæ•° (N)", min_value=0, value=1)
    target_user = st.text_input("å‰”é™¤å•ä¸€è¯„è®ºç”¨æˆ·", value="false")
    st.markdown("---")
    run_btn = st.button("ğŸš€ å¼€å§‹ä¸¥æ ¼æŠ“å–", type="primary", use_container_width=True)

tab1, tab2 = st.tabs(["ğŸ“‹ ç»“æœåˆ—è¡¨", "ğŸ•’ å†å²è®°å½•"])

with tab1:
    if run_btn:
        if start_p > end_p:
            st.error("âŒ èµ·å§‹é¡µç ä¸èƒ½å¤§äºç»“æŸé¡µç ")
        else:
            with st.spinner(f'æ­£åœ¨å¯¹ç¬¬ {start_p}-{end_p} é¡µè¿›è¡Œå…¨é‡æ£€æŸ¥...'):
                data = scrape_logic_strict(start_p, end_p, min_c, target_user)
            
            if data:
                df = pd.DataFrame(data)
                save_history({
                    "criteria": f"Page: {start_p}-{end_p} | Min: {min_c}",
                    "count": len(data),
                    "data": data
                })
                
                st.success(f"âœ… æ‰¾åˆ° {len(data)} ä¸ªæ¡ˆä¾‹ï¼")
                st.data_editor(
                    df,
                    column_config={
                        "é“¾æ¥": st.column_config.LinkColumn("ç‚¹å‡»æŸ¥çœ‹"),
                        "è¯„è®ºæ•°": st.column_config.NumberColumn("çƒ­åº¦", format="%d ğŸ’¬"),
                    },
                    hide_index=True,
                    use_container_width=True
                )
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (CSV)", csv, "gooood_strict_results.csv", "text/csv")
            else:
                st.warning("âš ï¸ åœ¨æŒ‡å®šèŒƒå›´å†…æœªæ‰¾åˆ°æ»¡è¶³è¯„è®ºæ•°è¦æ±‚çš„æ¡ˆä¾‹ã€‚")

with tab2:
    st.header("å†å²è®°å½•")
    history_data = load_history()
    if not history_data:
        st.caption("æš‚æ— å†å²è®°å½•")
    else:
        for i, record in enumerate(history_data):
            with st.expander(f"ğŸ“… {record['saved_at']} - {record.get('criteria','')} (ç»“æœ: {record['count']})"):
                if record['data']:
                    st.dataframe(pd.DataFrame(record['data']), hide_index=True)
