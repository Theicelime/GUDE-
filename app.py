import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import os
import json
import re

# --- é…ç½®éƒ¨åˆ† ---
HISTORY_FILE = "search_history.json"
BASE_URL = "https://www.gooood.cn"

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´ (é˜²æ­¢è¢«åçˆ¬)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
}

# --- é€»è¾‘å¤„ç†å‡½æ•° (åŸºäºä½ çš„å‚è€ƒä»£ç ç§»æ¤) ---

def load_history():
    """åŠ è½½å†å²è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(record):
    """ä¿å­˜å†å²è®°å½•"""
    history = load_history()
    # æ·»åŠ æ—¶é—´æˆ³
    record['saved_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    history.insert(0, record)  # æ’å…¥åˆ°æœ€å‰é¢
    # é™åˆ¶å†å²è®°å½•æ•°é‡ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§
    if len(history) > 20:
        history = history[:20]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def parse_date(date_str):
    """è§£ææ—¥æœŸï¼Œå…¼å®¹ gooood çš„æ ¼å¼"""
    if not date_str: return None
    try:
        # æ¸…ç†å­—ç¬¦ä¸²ï¼Œgooood æœ‰æ—¶ç”¨ . æœ‰æ—¶ç”¨ -
        date_str = date_str.strip().replace('.', '-')
        return datetime.strptime(date_str, "%Y-%m-%d").date()
    except:
        return None

def has_brackets(title):
    """
    [ç§»æ¤åŠŸèƒ½] æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«æ‹¬å·
    æ”¯æŒä¸­æ–‡æ‹¬å·ï¼ˆï¼‰å’Œè‹±æ–‡æ‹¬å·()
    """
    if not title:
        return False
    # æ£€æŸ¥ä¸­æ–‡æ‹¬å·
    chinese_brackets = re.search(r'ï¼ˆ[^ï¼‰]*ï¼‰', title)
    # æ£€æŸ¥è‹±æ–‡æ‹¬å·
    english_brackets = re.search(r'\([^)]*\)', title)
    return chinese_brackets is not None or english_brackets is not None

def contains_chinese(text):
    """[ç§»æ¤åŠŸèƒ½] æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    if not text:
        return False
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
    return bool(chinese_pattern.search(text))

def check_comments_deeply(article_url, target_user="false"):
    """
    [æ ¸å¿ƒé€»è¾‘] è¿›å…¥æ–‡ç« è¯¦æƒ…é¡µï¼š
    1. è·å–çœŸå®è¯„è®ºæ•°
    2. æ£€æŸ¥æ˜¯å¦åªæœ‰ 'false' ç”¨æˆ·è¯„è®º
    è¿”å›: (æ˜¯å¦ä¿ç•™, çœŸå®æ•°é‡, å¤‡æ³¨ä¿¡æ¯)
    """
    try:
        # éšæœºå»¶æ—¶ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«è¢«å°
        time.sleep(random.uniform(0.5, 1.2))
        
        resp = requests.get(article_url, headers=HEADERS, timeout=10)
        if resp.status_code != 200:
            return False, 0, "æ— æ³•è®¿é—®è¯¦æƒ…é¡µ"
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # --- æå–è¯„è®ºè€…é€»è¾‘ ---
        # è¯´æ˜ï¼šWordpress å¸¸è§çš„è¯„è®ºè€… class æ˜¯ .fn æˆ– .comment-author
        # å¦‚æœ gooood æ”¹ç‰ˆï¼Œè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´ CSS é€‰æ‹©å™¨
        comment_elements = soup.select('.comment-body') 
        
        authors = []
        for c in comment_elements:
            # å°è¯•è·å–ä½œè€…å
            author_tag = c.select_one('.fn') or c.select_one('.comment-author')
            if author_tag:
                authors.append(author_tag.get_text(strip=True))
        
        real_count = len(authors)
        
        if real_count == 0:
            return False, 0, "è¯¦æƒ…é¡µæ— è¯„è®º"

        # --- "false" ç”¨æˆ·æ’æŸ¥é€»è¾‘ ---
        # ä½ çš„éœ€æ±‚ï¼šå¦‚æœåªæœ‰è¿™ä¸ªç”¨æˆ·è¯„è®ºï¼Œåˆ é™¤
        unique_authors = set(authors)
        
        # å¦‚æœå»é‡åçš„ä½œè€…åªæœ‰ "false" (ä¸åŒºåˆ†å¤§å°å†™)ï¼Œåˆ™è§†ä¸ºæ— æ•ˆ
        if len(unique_authors) == 1 and target_user.lower() in [u.lower() for u in unique_authors]:
            return False, real_count, f"ä»…åŒ…å«ç”¨æˆ· {target_user}ï¼Œå·²å‰”é™¤"
            
        return True, real_count, "æœ‰æ•ˆæ¡ˆä¾‹"

    except Exception as e:
        return False, 0, f"è§£æé”™è¯¯: {str(e)}"

def scrape_logic(start_date, end_date, min_comments, target_user_filter):
    """
    ä¸»çˆ¬è™«é€»è¾‘
    """
    results = []
    page = 1
    keep_scraping = True
    
    # UI å ä½ç¬¦
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    # è½¬æ¢æ—¥æœŸæ ¼å¼ä»¥ä¾¿æ¯”è¾ƒ
    start_date = pd.to_datetime(start_date).date()
    end_date = pd.to_datetime(end_date).date()

    while keep_scraping:
        # æ„å»º URL
        url = f"{BASE_URL}/page/{page}" if page > 1 else BASE_URL
        status_text.markdown(f"**æ­£åœ¨æ‰«æç¬¬ {page} é¡µ...**")
        
        try:
            resp = requests.get(url, headers=HEADERS, timeout=15)
            if resp.status_code != 200:
                st.warning(f"é¡µé¢ {url} æ— æ³•è®¿é—®ï¼Œçˆ¬è™«åœæ­¢ã€‚")
                break
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # è·å–æ–‡ç« åˆ—è¡¨ (Gooood å¸¸è§çš„æ–‡ç« å®¹å™¨ class æ˜¯ .post)
            articles = soup.select('.post') 
            
            if not articles:
                st.info("æœªæ‰¾åˆ°æ›´å¤šæ–‡ç« ï¼Œå·²åˆ°è¾¾æœ«å°¾ã€‚")
                break

            for i, article in enumerate(articles):
                # 1. æå–æ—¥æœŸ
                date_tag = article.select_one('.time') or article.select_one('.entry-date')
                if not date_tag: continue
                
                article_date = parse_date(date_tag.get_text())
                if not article_date: continue

                # 2. æ—¥æœŸèŒƒå›´åˆ¤æ–­
                if article_date > end_date:
                    continue # å¤ªæ–°äº†ï¼Œè·³è¿‡ï¼Œç»§ç»­æ‰¾åŒä¸€é¡µçš„ä¸‹ä¸€ä¸ª
                if article_date < start_date:
                    keep_scraping = False # å¤ªæ—§äº†ï¼Œæ•´ä¸ªå¾ªç¯ç»“æŸ
                    break
                
                # 3. æå–æ ‡é¢˜å’Œé“¾æ¥
                title_tag = article.select_one('h2 a') or article.select_one('h1 a')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag['href']

                # --- ç§»æ¤çš„è¿‡æ»¤é€»è¾‘ ---
                # A. å¿…é¡»åŒ…å«ä¸­æ–‡
                if not contains_chinese(title):
                    continue
                # B. ä¸èƒ½åŒ…å«æ‹¬å·
                if has_brackets(title):
                    # print(f"è¿‡æ»¤æ‰æ‹¬å·æ ‡é¢˜: {title}")
                    continue

                # 4. åˆæ­¥è¯„è®ºæ•°ç­›é€‰ (åœ¨åˆ—è¡¨é¡µå¿«é€Ÿç­›é€‰)
                # åˆ—è¡¨é¡µé€šå¸¸æ˜¾ç¤º "15 Comments"
                raw_comment_count = 0
                comment_tag = article.select_one('.comments-link')
                if comment_tag:
                    txt = comment_tag.get_text()
                    # æå–æ•°å­—
                    nums = re.findall(r'\d+', txt)
                    if nums:
                        raw_comment_count = int(nums[0])
                
                # åªæœ‰åˆ—è¡¨é¡µæ˜¾ç¤ºçš„è¯„è®ºæ•° > Nï¼Œæ‰è¿›å»ç»†æŸ¥
                # ä¼˜åŒ–ï¼šå¦‚æœ min_comments å¾ˆå°ï¼Œå¯èƒ½ä¸éœ€è¦è¿™ä¸€æ­¥ï¼Œä½†ä¸ºäº†æ•ˆç‡è¿˜æ˜¯åŠ ä¸Š
                if raw_comment_count >= min_comments:
                    
                    status_text.text(f"æ­£åœ¨æ·±åº¦æ£€æŸ¥: {title[:20]}...")
                    
                    # è¿›å…¥è¯¦æƒ…é¡µæ£€æŸ¥ (æ£€æŸ¥æ˜¯å¦æœ‰ false ç”¨æˆ·)
                    is_valid, final_count, note = check_comments_deeply(link, target_user_filter)
                    
                    if is_valid and final_count >= min_comments:
                        results.append({
                            "æ—¥æœŸ": str(article_date),
                            "æ ‡é¢˜": title,
                            "é“¾æ¥": link,
                            "è¯„è®ºæ•°": final_count,
                            "çŠ¶æ€": note
                        })
            
            # æ›´æ–°è¿›åº¦æ¡ (æ¨¡æ‹Ÿæ•ˆæœ)
            if page % 5 == 0:
                progress_bar.progress(min(page / 50, 1.0))
                
            page += 1
            # ç®€å•çš„é˜²å°å»¶æ—¶
            time.sleep(1)
            
        except Exception as e:
            st.error(f"æŠ“å–ä¸­æ–­: {e}")
            break
            
    progress_bar.progress(100)
    status_text.success("æŠ“å–å®Œæˆï¼")
    return results

# --- Streamlit ç•Œé¢æ„å»º ---

st.set_page_config(page_title="Gooood æ¡ˆä¾‹ç­›é€‰å™¨ (Webç‰ˆ)", layout="wide", page_icon="ğŸ›ï¸")

st.title("ğŸ›ï¸ Gooood.cn æ¡ˆä¾‹ç­›é€‰å·¥å…·")
st.markdown("""
è¿™æ˜¯ä¸€ä¸ªåŸºäº **Python Streamlit** çš„ Web å·¥å…·ï¼Œç§»æ¤äº†åŸæœ‰çš„ç­›é€‰é€»è¾‘ï¼š
1. **æ—¥æœŸç­›é€‰**ï¼šç²¾å‡†å®šä½æ—¶é—´æ®µã€‚
2. **æ ‡é¢˜æ¸…æ´—**ï¼šè‡ªåŠ¨å‰”é™¤ä¸å«ä¸­æ–‡æˆ–åŒ…å«æ‹¬å· `()` `ï¼ˆï¼‰` çš„æ ‡é¢˜ã€‚
3. **ç”¨æˆ·é»‘åå•**ï¼šè‡ªåŠ¨å‰”é™¤ä»…ç”±æŒ‡å®šç”¨æˆ·ï¼ˆå¦‚ `false`ï¼‰è¯„è®ºçš„æ¡ˆä¾‹ã€‚
""")

# --- ä¾§è¾¹æ ï¼šè®¾ç½® ---
with st.sidebar:
    st.header("ğŸ› ï¸ ç­›é€‰æ¡ä»¶è®¾ç½®")
    
    # æ—¥æœŸè®¾ç½®
    col_d1, col_d2 = st.columns(2)
    start_d = col_d1.date_input("å¼€å§‹æ—¥æœŸ", value=pd.to_datetime("2023-01-01"))
    end_d = col_d2.date_input("ç»“æŸæ—¥æœŸ", value=datetime.now())
    
    # è¯„è®ºæ•°è®¾ç½®
    min_c = st.number_input("æœ€å°è¯„è®ºæ•° (N)", min_value=0, value=5, help="åªæœ‰å¤§äºç­‰äºæ­¤æ•°é‡çš„æ¡ˆä¾‹æ‰ä¼šè¢«ä¿ç•™")
    
    # ç”¨æˆ·è¿‡æ»¤è®¾ç½®
    target_user = st.text_input("å‰”é™¤å•ä¸€è¯„è®ºç”¨æˆ·", value="false", help="å¦‚æœæŸæ¡ˆä¾‹çš„æ‰€æœ‰è¯„è®ºéƒ½ä»…æ¥è‡ªæ­¤ç”¨æˆ·åï¼Œè¯¥æ¡ˆä¾‹å°†è¢«å‰”é™¤ã€‚")
    
    st.markdown("---")
    run_btn = st.button("ğŸš€ å¼€å§‹æŠ“å–", type="primary", use_container_width=True)

# --- ä¸»ç•Œé¢ï¼šç»“æœå±•ç¤º ---

tab1, tab2 = st.tabs(["ğŸ“‹ å½“å‰æŸ¥è¯¢ç»“æœ", "ğŸ•’ å†å²è®°å½•"])

with tab1:
    if run_btn:
        if start_d > end_d:
            st.error("âŒ é”™è¯¯ï¼šå¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸï¼")
        else:
            with st.spinner('æ­£åœ¨è¿æ¥ gooood.cn è¿›è¡Œæ•°æ®æŠ“å–ä¸åˆ†æï¼Œè¯·ç¨å€™...'):
                # è¿è¡Œçˆ¬è™«
                data = scrape_logic(start_d, end_d, min_c, target_user)
            
            if data:
                df = pd.DataFrame(data)
                
                # ä¿å­˜æœ¬æ¬¡ç»“æœåˆ°å†å²
                save_history({
                    "criteria": f"{start_d} ~ {end_d} | Min: {min_c}",
                    "count": len(data),
                    "data": data
                })
                
                st.success(f"âœ… æˆåŠŸæ‰¾åˆ° {len(data)} ä¸ªç¬¦åˆæ¡ä»¶çš„æ¡ˆä¾‹ï¼")
                
                # æ˜¾ç¤ºäº¤äº’å¼è¡¨æ ¼
                st.data_editor(
                    df,
                    column_config={
                        "é“¾æ¥": st.column_config.LinkColumn("ç‚¹å‡»è·³è½¬"),
                        "è¯„è®ºæ•°": st.column_config.NumberColumn("è¯„è®ºçƒ­åº¦", format="%d ğŸ’¬"),
                    },
                    hide_index=True,
                    use_container_width=True
                )
                
                # ä¸‹è½½æŒ‰é’®åŒº
                c1, c2 = st.columns(2)
                # CSV ä¸‹è½½
                csv = df.to_csv(index=False).encode('utf-8-sig')
                c1.download_button("ğŸ“¥ ä¸‹è½½ä¸º CSV", csv, "gooood_cases.csv", "text/csv", use_container_width=True)
                
                # Excel ä¸‹è½½ (éœ€è¦ openpyxl)
                # ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œæ¼”ç¤º CSVï¼Œå¦‚æœéœ€è¦ Excelï¼Œéœ€ç¡®ä¿å®‰è£… openpyxl å¹¶ä½¿ç”¨ pd.to_excel
            else:
                st.warning("âš ï¸ åœ¨æŒ‡å®šæ¡ä»¶ä¸‹æœªæ‰¾åˆ°ä»»ä½•æ¡ˆä¾‹ã€‚")

with tab2:
    st.header("å†å²æŸ¥è¯¢è®°å½•")
    history_data = load_history()
    
    if not history_data:
        st.caption("æš‚æ— å†å²è®°å½•")
    
    for i, record in enumerate(history_data):
        with st.expander(f"ğŸ“… {record['saved_at']} - æ‰¾åˆ° {record['count']} ä¸ªæ¡ˆä¾‹"):
            st.caption(f"ç­›é€‰æ¡ä»¶: {record['criteria']}")
            if record['data']:
                h_df = pd.DataFrame(record['data'])
                st.dataframe(
                    h_df,
                    column_config={"é“¾æ¥": st.column_config.LinkColumn("é“¾æ¥")},
                    hide_index=True,
                    use_container_width=True
                )
                # å†å²è®°å½•ä¸‹è½½
                h_csv = h_df.to_csv(index=False).encode('utf-8-sig')
                st.download_button(f"ä¸‹è½½æ­¤è®°å½• (CSV)", h_csv, key=f"hist_{i}")
