import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
from datetime import datetime
import os
import json
import re

# --- é…ç½®éƒ¨åˆ† ---
HISTORY_FILE = "search_history.json"
BASE_URL = "https://www.gooood.cn"

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´ (æ¨¡æ‹Ÿ Chrome 120)
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Referer": "https://www.gooood.cn/"
}

# --- è¾…åŠ©å‡½æ•° ---

def load_history():
    """åŠ è½½å†å²è®°å½•"""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return []
    return []

def save_history(record):
    """ä¿å­˜å†å²è®°å½•"""
    history = load_history()
    record['saved_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    history.insert(0, record)
    if len(history) > 20: 
        history = history[:20]
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def has_brackets(title):
    """æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«æ‹¬å· (æ”¯æŒä¸­æ–‡å’Œè‹±æ–‡)"""
    if not title: return False
    return (re.search(r'ï¼ˆ[^ï¼‰]*ï¼‰', title) is not None or 
            re.search(r'\([^)]*\)', title) is not None)

def contains_chinese(text):
    """æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡"""
    if not text: return False
    return bool(re.search(r'[\u4e00-\u9fff]+', text))

def get_list_page_comment_count(article_soup):
    """
    [ä¿®å¤æ ¸å¿ƒ] å°è¯•ä»åˆ—è¡¨é¡µçš„æ–‡ç« å¡ç‰‡ä¸­æå–è¯„è®ºæ•°
    ä½¿ç”¨äº†å¤šç§ç­–ç•¥ï¼Œé˜²æ­¢æ¼æŠ“
    è¿”å›: (int æ•°é‡, bool æ˜¯å¦æˆåŠŸæ‰¾åˆ°)
    """
    try:
        # ç­–ç•¥ 1: æ ‡å‡† class (.comments-link)
        tag = article_soup.select_one('.comments-link')
        if tag:
            nums = re.findall(r'\d+', tag.get_text())
            if nums: return int(nums[0]), True

        # ç­–ç•¥ 2: æŸ¥æ‰¾é“¾æ¥ä¸­åŒ…å« #comments çš„ (é€šå¸¸æ˜¯è¯„è®ºé“¾æ¥)
        links = article_soup.select('a[href*="#comments"]')
        for link in links:
            txt = link.get_text()
            # æ’é™¤ "Add a comment" è¿™ç§æ²¡æœ‰æ•°å­—çš„
            nums = re.findall(r'\d+', txt)
            if nums: return int(nums[0]), True

        # ç­–ç•¥ 3: æŸ¥æ‰¾æ–‡æœ¬ä¸­åŒ…å« "è¯„è®º" æˆ– "Comment" çš„ä»»ä½•å°å­—
        meta_tags = article_soup.select('.post-meta, .entry-meta, .meta-info')
        for meta in meta_tags:
            txt = meta.get_text()
            if "è¯„è®º" in txt or "Comment" in txt:
                nums = re.findall(r'\d+', txt)
                if nums: return int(nums[0]), True
        
        # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œè¿”å› 0ï¼Œä¸”æ ‡è®°ä¸º False (æ²¡æ‰¾åˆ°æ˜ç¡®æ•°å­—)
        return 0, False
    except:
        return 0, False

def check_comments_deeply(article_url, target_user="false"):
    """
    [è¯¦æƒ…é¡µé€»è¾‘] è¿›å…¥æ–‡ç« è¯¦æƒ…é¡µï¼šè·å–çœŸå®è¯„è®ºæ•° + é»‘åå•è¿‡æ»¤
    """
    try:
        time.sleep(random.uniform(0.3, 0.8)) # ç¨å¾®åŠ å¿«ä¸€ç‚¹é€Ÿåº¦
        
        resp = requests.get(article_url, headers=HEADERS, timeout=15)
        if resp.status_code != 200:
            return False, 0, "æ— æ³•è®¿é—®è¯¦æƒ…é¡µ"
        
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # æå–è¯„è®ºåŒºåŸŸ (é€‚é…å¤šç§ç»“æ„)
        # .comment-body æ˜¯æ­£æ–‡, .comment-list li æ˜¯åˆ—è¡¨é¡¹
        comment_elements = soup.select('.comment-body') 
        if not comment_elements:
            comment_elements = soup.select('li.comment')
        
        authors = []
        for c in comment_elements:
            # å°è¯•è·å–ä½œè€…å
            author_tag = c.select_one('.fn') or c.select_one('.comment-author') or c.select_one('.url')
            if author_tag:
                authors.append(author_tag.get_text(strip=True))
        
        real_count = len(authors)
        
        if real_count == 0:
            return False, 0, "è¯¦æƒ…é¡µæ— è¯„è®º"

        # --- "false" ç”¨æˆ·æ’æŸ¥é€»è¾‘ ---
        unique_authors = set(authors)
        if len(unique_authors) == 1 and target_user.lower() in [u.lower() for u in unique_authors]:
            return False, real_count, f"ä»…åŒ…å«ç”¨æˆ· {target_user}ï¼Œå·²å‰”é™¤"
            
        return True, real_count, "æœ‰æ•ˆæ¡ˆä¾‹"

    except Exception as e:
        return False, 0, f"è¯¦æƒ…é¡µè§£æè¯¯: {str(e)}"

def scrape_logic_by_pages(start_page, end_page, min_comments, target_user_filter):
    """
    [ä¸»å¾ªç¯]
    """
    results = []
    
    status_text = st.empty()
    progress_bar = st.progress(0)
    
    total_pages = end_page - start_page + 1
    
    # ç»Ÿè®¡æ•°æ®
    stats = {"scanned": 0, "deep_checked": 0, "found": 0}
    
    for i, page in enumerate(range(start_page, end_page + 1)):
        
        progress_percentage = (i) / total_pages
        progress_bar.progress(progress_percentage)
        
        url = f"{BASE_URL}/page/{page}" if page > 1 else BASE_URL
        status_text.markdown(f"**ğŸ“¡ æ­£åœ¨æ‰«æç¬¬ {page} é¡µ...** (å·²æ‰¾åˆ°: {stats['found']} ä¸ª)")
        
        try:
            resp = requests.get(url, headers=HEADERS, timeout=20)
            if resp.status_code != 200:
                st.warning(f"âš ï¸ ç¬¬ {page} é¡µè¿”å›çŠ¶æ€ç  {resp.status_code}ï¼Œè·³è¿‡ã€‚")
                continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            articles = soup.select('.post') 
            
            if not articles:
                # å°è¯•å¤‡ç”¨é€‰æ‹©å™¨ï¼Œé˜²æ­¢ Gooood æ”¹ç‰ˆ
                articles = soup.select('article')
            
            if not articles:
                st.warning(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ°æ–‡ç« å…ƒç´ ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜æˆ–åˆ°è¾¾æœ«å°¾ã€‚")
                continue

            for article in articles:
                stats["scanned"] += 1
                
                # 1. æå–æ ‡é¢˜å’Œé“¾æ¥
                title_tag = article.select_one('h2 a') or article.select_one('h1 a') or article.select_one('.entry-title a')
                if not title_tag: continue
                
                title = title_tag.get_text(strip=True)
                link = title_tag['href']

                # 2. æ ‡é¢˜æ¸…æ´—
                if not contains_chinese(title): continue
                if has_brackets(title): continue

                # 3. è¯„è®ºæ•°åˆç­› (åˆ—è¡¨é¡µ) - æ ¸å¿ƒä¿®å¤éƒ¨åˆ†
                raw_count, found_on_list = get_list_page_comment_count(article)
                
                # å†³ç­–é€»è¾‘ï¼š
                # A. å¦‚æœåˆ—è¡¨é¡µæ˜ç¡®æ˜¾ç¤ºæ•°é‡ >= minï¼Œå½“ç„¶è¦æŸ¥ã€‚
                # B. å¦‚æœåˆ—è¡¨é¡µæ˜ç¡®æ˜¾ç¤ºæ•°é‡ < min (ä¸”ä¸ä¸º0)ï¼Œé‚£å°±ä¸æŸ¥ã€‚
                # C. [å…³é”®] å¦‚æœåˆ—è¡¨é¡µæ²¡æ‰¾åˆ°æ•°å­— (found_on_list is False)ï¼Œæˆ–è€…æ˜¯ 0ï¼Œ
                #    ä¸ºäº†é˜²æ­¢æ¼æŠ“ï¼Œæˆ‘ä»¬å‡è®¾å®ƒå¯èƒ½æœ‰è¯„è®ºï¼Œå¼ºåˆ¶æŸ¥è¯¦æƒ…é¡µï¼
                
                should_deep_check = False
                
                if found_on_list:
                    if raw_count >= min_comments:
                        should_deep_check = True
                else:
                    # åˆ—è¡¨é¡µæ²¡è¯»å‡ºæ¥ï¼Œæˆ–è€…è¯»å‡ºæ¥æ˜¯0ä½†å¯èƒ½æ˜¯è¯¯è¯» -> å¼ºåˆ¶æ£€æŸ¥
                    # é™¤éç”¨æˆ·è®¾ç½®çš„é˜ˆå€¼æé«˜(æ¯”å¦‚50)ï¼Œå¦åˆ™éƒ½è¿›å»çœ‹çœ‹ï¼Œä¿è¯ä¸æ¼
                    should_deep_check = True 

                if should_deep_check:
                    # status_text.text(f"ğŸ” æ£€æŸ¥è¯¦æƒ…: {title[:15]}...") # å‡å°‘UIåˆ·æ–°é¢‘ç‡æé€Ÿ
                    stats["deep_checked"] += 1
                    
                    is_valid, final_count, note = check_comments_deeply(link, target_user_filter)
                    
                    if is_valid and final_count >= min_comments:
                        results.append({
                            "é¡µç ": page,
                            "æ ‡é¢˜": title,
                            "é“¾æ¥": link,
                            "è¯„è®ºæ•°": final_count,
                            "çŠ¶æ€": note
                        })
                        stats["found"] += 1
            
            # ç®€å•çš„é˜²å°å»¶æ—¶
            time.sleep(0.5)
            
        except Exception as e:
            st.error(f"ç¬¬ {page} é¡µç³»ç»Ÿé”™è¯¯: {e}")
            
    progress_bar.progress(100)
    status_text.success(f"å®Œæˆï¼æ‰«æ {stats['scanned']} ç¯‡ï¼Œæ·±åº¦æ£€æŸ¥ {stats['deep_checked']} ç¯‡ï¼Œå‘½ä¸­ {stats['found']} ç¯‡ã€‚")
    return results

# --- Streamlit ç•Œé¢ ---

st.set_page_config(page_title="Gooood æ¡ˆä¾‹ç­›é€‰ (ä¿®å¤ç‰ˆ)", layout="wide", page_icon="ğŸ›ï¸")

st.title("ğŸ›ï¸ Gooood.cn æ¡ˆä¾‹ç­›é€‰å·¥å…· (ä¿®å¤ç‰ˆ)")
st.markdown("""
**æœ¬æ¬¡æ›´æ–°ä¿®å¤äº†æ¼æŠ“é—®é¢˜**ï¼šå¦‚æœåˆ—è¡¨é¡µæ— æ³•è¯»å–è¯„è®ºæ•°ï¼Œå°†å¼ºåˆ¶è¿›å…¥è¯¦æƒ…é¡µæ£€æŸ¥ï¼Œç¡®ä¿ä¸é”™è¿‡ä»»ä½•ä¸€æ¡è¯„è®ºã€‚
""")

with st.sidebar:
    st.header("ğŸ› ï¸ ç­›é€‰è®¾ç½®")
    col_p1, col_p2 = st.columns(2)
    start_p = col_p1.number_input("èµ·å§‹é¡µç ", min_value=1, value=800, step=1)
    end_p = col_p2.number_input("ç»“æŸé¡µç ", min_value=1, value=805, step=1)
    
    st.markdown("---")
    min_c = st.number_input("æœ€å°è¯„è®ºæ•° (N)", min_value=0, value=1)
    target_user = st.text_input("å‰”é™¤å•ä¸€è¯„è®ºç”¨æˆ·", value="false")
    st.markdown("---")
    run_btn = st.button("ğŸš€ å¼€å§‹æŠ“å–", type="primary", use_container_width=True)

tab1, tab2 = st.tabs(["ğŸ“‹ ç»“æœåˆ—è¡¨", "ğŸ•’ å†å²è®°å½•"])

with tab1:
    if run_btn:
        if start_p > end_p:
            st.error("âŒ èµ·å§‹é¡µç ä¸èƒ½å¤§äºç»“æŸé¡µç ")
        else:
            with st.spinner(f'æ­£åœ¨æ·±åº¦æ‰«æç¬¬ {start_p} åˆ° {end_p} é¡µ...'):
                data = scrape_logic_by_pages(start_p, end_p, min_c, target_user)
            
            if data:
                df = pd.DataFrame(data)
                save_history({
                    "criteria": f"Page: {start_p}-{end_p} | Min: {min_c}",
                    "count": len(data),
                    "data": data
                })
                
                st.success(f"âœ… æ‰¾åˆ° {len(data)} ä¸ªæ¡ˆä¾‹ï¼")
                st.data_editor(
                    df,
                    column_config={
                        "é“¾æ¥": st.column_config.LinkColumn("ç‚¹å‡»æŸ¥çœ‹"),
                        "è¯„è®ºæ•°": st.column_config.NumberColumn("çƒ­åº¦", format="%d ğŸ’¬"),
                    },
                    hide_index=True,
                    use_container_width=True
                )
                csv = df.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ (CSV)", csv, "gooood_results.csv", "text/csv")
            else:
                st.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¡ˆä¾‹ã€‚å¦‚æœç¡®è®¤æœ‰ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæ˜¯å¦é€šç•…ã€‚")

with tab2:
    st.header("å†å²è®°å½•")
    history_data = load_history()
    if not history_data:
        st.caption("æš‚æ— å†å²è®°å½•")
    else:
        for i, record in enumerate(history_data):
            with st.expander(f"ğŸ“… {record['saved_at']} - {record.get('criteria','')} (ç»“æœ: {record['count']})"):
                if record['data']:
                    st.dataframe(pd.DataFrame(record['data']), hide_index=True)
